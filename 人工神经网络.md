[toc]


# 神经网络bp算法

三个关键字：`2-2-1`,`微分`,`路径`

`2-2-1`：最小原型神经网络结构

`微分`: 神经网络参数优化方式

`路径`：神经网络的参数在几何结构上对网络输出产生影响的方式，根据路径可以直接写出参数迭代方式

## 1.微分求极值

机器学习问题大多数都可以抽象为 `极值问题`

极值点的一阶导数必然是`0`，叫零点

以数值方式求解零点：

1. 任选一个点
2. 用合适的步长移动
3. 移动到一阶导数为0的地方，求函数极大值，自变量沿着一阶导数的方向移动；求函数极小值，自变量沿着一阶导数的反方向移动

> 实例

求 `$y = (x - 2)^2$`的极小值。`$y' = 2(x-2)$`，迭代公式为：

```math
x = x - \eta * y' = x - 2 \eta (x-2)
```

其中：`$\eta$`是学习速率，是一个`(0,1)`区间的实数。学习速率不能太大

![image.png](https://note.youdao.com/yws/res/32091/WEBRESOURCE91dd23fe5ba44710aeadc3b966603f00)

## 2.一个最简神经网络的BP算法推导

> 实例：前馈神经网络演示 BP 算法推导过程

![image.png](https://note.youdao.com/yws/res/32092/WEBRESOURCEd7ad398edbc24a85b804c3ef6c636fd9)

> 分析

一个输入层，一个隐层，一个输出层。输入层有两个神经元，`n1`和`n2`，隐层有两个神经元，`n3`,`n4`，输出层有一个神经元，`n5`


> 注释：

1. `n1`的输出是`x1`，++权重++`$\omega$`
2. `b1`是神经元`n3`的++偏差++，神经元`n3`的输入是`z1`，`z1`是神经元`n3`的所有输入`x1`,`x2`,`b1`的线性组合，是一个中间变量。神经元的输出为`o1`
3. `f(x)`：神经元激活函数

假设一个样本，它有两个特征`[x1,x2]`，目标值是`y`：
**神经网络的学习过程，就是多次调整权重和偏差，使得神经网络输入是`[x1,x2]`的时候，输出值尽可能接近`y`**

有如下结果：

```math
z_1 = x_1 \omega_1 + x_2 \omega_3 + b_1
```
```math
o_1 = f(z_1)
```
```math
z_2 = x_1 \omega_2 + x_2 \omega_4 + b_4
```
```math
o_2 = f(z_2)
```
```math
z_3 = o_1 \omega_5 + o_2 \omega_6 + b_3
```
```math
o_3 = f(z_3)
```

其中`$f(x)$`是神经元的激活函数。

特殊的激活函数：`sigmod`函数

```math
f(x) = \dfrac{e^x}{1 + e^x}
```

此时`sigmod`函数的一阶导数为：

```math
f(x)' = f(x)(1 - f(x))
```

**用`Err`衡量它们的差异**，训练神经网络即是让它们之间的差异越来越小

```math
Err = \dfrac {1}{2}(o_3 - y)^2
```

神经网络的训练，**是一个求`Err`极小值的问题**。即每一轮训练，都是计算`Err`对`$\omega_1$`、`$\omega_2$`、`$\omega_3$`、`$\omega_4$`、`$\omega_5$`、`$b_1$`、`$b_2$`、`$b_3$`的一阶偏导，然后迭代更新

迭代公式如下:

```math
\omega_5 = \omega_5 - \eta \dfrac {\partial Err}{\partial \omega_5}
```

```math
\omega_6 = \omega_6 - \eta \dfrac {\partial Err}{\partial \omega_6}
```


```math
b_3 = b_3 - \eta \dfrac {\partial Err}{\partial b_3}
```

其中：
```math
\dfrac {\partial Err}{\partial \omega_5} = (o_3 - y)f(z_3)^{'}o_1
```

> 分析

1. `f(x)`是`sigmod`函数时， `$f(z_3)^{'}$` 和 `$o_1$` 都是正数，且取值范围在`(0,1)`区域
2. 如果神经元激活函数为`sigmod`函数，**如果神经网络输出值比目标值大，迭代会让权重变小一些，如果神经网络输出值比目标值小，迭代会让权重都变大一些**
3. **每个权重的迭代，受到三个影响：神经网络输出误差`$o_3 - y$`；跟权重相连的前一个神经元的输出值；跟权重相连的后一个神经元的激活函数的一阶导数**
4. `Err`对`$\omega_6$`的一阶偏导：

```math
\dfrac {\partial Err}{\partial \omega_6} = (o_3 - y)f(z_3)^{'}o_2
```

5. `Err`对`$b_3$`的一阶偏导：

```math
\dfrac {\partial Err}{\partial b_3} = (o_3 - y)f(z_3)^{'}
```

隐藏层的参数`$\omega_1$`,`$\omega_2$`,`$\omega_3$`,`$\omega_4$`,`$b_1$`,`$b_2$`的迭代公式如下：

```math
\omega_1 = \omega_1 - \eta \dfrac {\partial Err}{\partial \omega_1}
```
```math
\omega_2 = \omega_2 - \eta \dfrac {\partial Err}{\partial \omega_2}
```
```math
\omega_3 = \omega_3 - \eta \dfrac {\partial Err}{\partial \omega_3}
```
```math
\omega_4 = \omega_4 - \eta \dfrac {\partial Err}{\partial \omega_4}
```
```math
b_1 = b_1 - \eta \dfrac {\partial Err}{\partial b_1}
```
```math
b_2 = b_2 - \eta \dfrac {\partial Err}{\partial b_2}
```

`Err`对`$\omega_1$`的一阶偏导：

```math
\dfrac {\partial Err}{\partial \omega_1} = (o_3 - y)f(z_3)^{'}f(z_1)^{'} \omega_5 x_1
```

分析：

1. `$\omega_1$`受`$o_3 - y$`、`$x_1$`、`$f(z_1)^{'}$`影响和前面类型
2. 从`$\omega_1$`到神经网络输出`o_3`的整条路径：`$x_1$`,`$f(z_1)$`,`$\omega_5$`,`$o_3$`,都对`$\omega_1$`产生影响，根据这条“路径”规则，可猜测其它权重的推导结果
3. 求导结果的相似性：`$\dfrac {\partial Err}{\partial \omega_1}$`、`$\dfrac {\partial Err}{\partial \omega_5}$`和`$\dfrac {\partial Err}{\partial \omega_6}$`，都有`$(o_3 - y) f(z_3)^{'}$`。所以可以从神经网络的后端向前计算，使用已经算好的结果。


`Err`对`$\omega_2$`的一阶偏导：

```math
\dfrac {\partial Err}{\partial \omega_2} = (o_3 - y)f(z_3)^{'}f(z_2)^{'} \omega_6 x_1
```

`Err`对`$\omega_3$`的一阶偏导：

```math
\dfrac {\partial Err}{\partial \omega_3} = (o_3 - y)f(z_3)^{'}f(z_1)^{'} \omega_5 x_2
```

`Err`对`$\omega_4$`的一阶偏导：

```math
\dfrac {\partial Err}{\partial \omega_4} = (o_3 - y)f(z_3)^{'}f(z_2)^{'} \omega_6 x_2
```

`Err`对`$b_1$`的一阶偏导：

```math
\dfrac {\partial Err}{\partial b_1} = (o_3 - y)f(z_3)^{'}f(z_1)^{'} \omega_5
```

`Err`对`$b_2$`的一阶偏导：

```math
\dfrac {\partial Err}{\partial b_2} = (o_3 - y)f(z_3)^{'}f(z_2)^{'} \omega_6
```

## 3.一个参数更少神经网络的 BP 算法推导

为减少参数，设置成每层神经网络的神经元共享一个`b`值。

![image.png](https://note.youdao.com/yws/res/32093/WEBRESOURCEfdaea7c88c2146a196b75275148a5763)

假设一个样本有两个特征`[x1,x2]`,样本标记`y`。从输入层到输出层，做一次前向传播：

```math
z_1 = x_1 \omega_1 + x_2 \omega_3 + b_1
```
```math
o_1 = f(z_1)
```
```math
z_2 = x_1 \omega_2 + x_2 \omega_4 + b_1
```
```math
o_2 = f(z_2)
```
```math
z_3 = o_1 \omega_5 + o_2 \omega_6 + b_2
```
```math
o_3 = f(z_3)
```

**输出层`$\omega_5$`、`$\omega_6$`、`$b_2$`的迭代公式如下：**

```math
\dfrac {\partial Err}{\partial \omega_5} = (o_3 - y)f(z_3)^{'}o_1
```

```math
\dfrac {\partial Err}{\partial \omega_6} = (o_3 - y)f(z_3)^{'}o_2
```

```math
\dfrac {\partial Err}{\partial b_2} = (o_3 - y)f(z_3)^{'}
```

**隐层的参数`$\omega_1$`,`$\omega_2$`,`$\omega_3$`,`$\omega_4$`,`$b_1$`的迭代公式如下：**

```math
\dfrac {\partial Err}{\partial \omega_1} = (o_3 - y)f(z_3)^{'}f(z_1)^{'} \omega_5 x_1
```

```math
\dfrac {\partial Err}{\partial \omega_2} = (o_3 - y)f(z_3)^{'}f(z_2)^{'} \omega_6 x_1
```

```math
\dfrac {\partial Err}{\partial \omega_3} = (o_3 - y)f(z_3)^{'}f(z_1)^{'} \omega_5 x_2
```

```math
\dfrac {\partial Err}{\partial \omega_4} = (o_3 - y)f(z_3)^{'}f(z_2)^{'} \omega_6 x_2
```

```math
\dfrac {\partial Err}{\partial b_1} = (o_3 - y)f(z_3)^{'}(f(z_1)^{'} \omega_5 + f(z_2)^{'} \omega_6)
```

> 注

偏差`$b_1$`是经过两条“路径”传递到`$o_3$`的，因此体现在推导结果，就是两个路径微分结果之和

> 实例

```cpp
#include <iostream>
#include <cmath>
#include <random>

double active(const double x) {
    return std::exp(x) / (1 + std::exp(x));
}

double dif_active(const double x) {
    return active(x) * (1 - active(x));
}

int main()
{
    //训练样本[x1,x2],y = [3,2],0.75
    constexpr double x1 = 3; constexpr double x2 = 2;
    constexpr double y = (x1 * x1 + 4 * x2 * x2 + 5) / 40;

    std::random_device rd;
    std::mt19937 gen(rd());
    double w1 = std::generate_canonical<double,10>(gen);
    double w2 = std::generate_canonical<double, 10>(gen);
    double w3 = std::generate_canonical<double, 10>(gen);
    double w4 = std::generate_canonical<double, 10>(gen);
    double w5 = std::generate_canonical<double, 10>(gen);
    double w6 = std::generate_canonical<double, 10>(gen);
    double b1 = std::generate_canonical<double, 10>(gen);
    double b2 = std::generate_canonical<double, 10>(gen);
    constexpr double eta = 0.1;

    constexpr int iter = 1000;

    for (int i = 0; i <= iter; i++) {
        //线性关系
        double z1 = x1 * w1 + x2 * w3 + b1;
        double o1 = active(z1);
        double z2 = x1 * w2 + x2 * w4 + b1;
        double o2 = active(z2);
        double z3 = o1 * w5 + o2 * w6 + b2;
        double o3 = active(z3);

        //误差
        double Err = 0.5 * (o3 - y) * (o3 - y);
        std::cout << "i = " << i << " o3 = " << o3 << " Err = " << Err << " w5 = " << w5 << std::endl;

        //迭代
        //输出层
        w5 = w5 - eta * (o3 - y) * dif_active(z3) * o1;
        w6 = w6 - eta * (o3 - y) * dif_active(z3) * o2;
        b2 = b2 - eta * (o3 - y) * dif_active(z3);
        //隐层
        w4 = w4 - eta * (o3 - y) * dif_active(z3) * dif_active(z2) * w6 * x2;
        w3 = w3 - eta * (o3 - y) * dif_active(z3) * dif_active(z1) * w5 * x2;
        w2 = w2 - eta * (o3 - y) * dif_active(z3) * dif_active(z2) * w6 * x1;
        w1 = w1 - eta * (o3 - y) * dif_active(z3) * dif_active(z1) * w5 * x1;
        b1 = b1 - eta * (o3 - y) * dif_active(z3) * (dif_active(z1) * w5 + dif_active(z2) * w6);
    }
}
```

![image.png](https://note.youdao.com/yws/res/32095/WEBRESOURCE0a079d8510094556a114e36a16a1bd99)

可以观察到训练次数达到一定次数，神经网络的输出非常接近样本目标值 `0.75`；可以发现，`o3`比`y`大的时候，`w5`是逐步减小的


## 4.两个隐层神经网络BP迭代公式的猜测与验证

![image.png](https://note.youdao.com/yws/res/32096/WEBRESOURCEfff41f80d190425a89f2b0b4b7b1cb73)

根据路径，从`$\omega_5$`到神经网络的输出`$o_6$`，只有一条路径：`$o_1$`、`$f(z_3)^{'}$`、`$\omega_11$`、`$f(z_6)^{'}$`、`$o_6 - y$`

所以 `Err`对 `$\omega_5$`的一阶偏导为：

```math
\dfrac {\partial Err}{\partial \omega_5} = (o_6 - y)f(z_6)^{'}f(z_3)^{'} \omega_{11} o_1
```

同理：`$\omega_6$`/`$\omega_7$`/`$\omega_8$`/`$\omega_9$`/`$\omega_10$`/`$\omega_{11}$`/`$\omega_{12}$`/`$\omega_{13}$`/`$b_3$`/`$b_2$`

`$\omega_1$`到`$o_6$`有三条路径，分别是：

```math
1)  x_1 , f(z_1) , \omega_5 , f(z_3) , \omega_{11} , f(z_6) , o_6-y
```

```math
2)  x_1 , f(z_1) , \omega_6 , f(z_4) , \omega_{12} , f(z_6) , o_6-y
```

```math
3)  x_1 , f(z_1) , \omega_7 , f(z_5) , \omega_{13} , f(z_6) , o_6-y
```

`$\omega_1$`的推导结果是三条路径的累加：

```math
\dfrac {\partial Err}{\partial \omega_1} = (o_6 - y)f(z_1)^{'}x_1 f(z_6)^{'}(f(z_3)^{'} \omega_{11}\omega_{5} + f(z_4)^{'} \omega_{12}\omega_{6} + f(z_5)^{'} \omega_{13}\omega_{7})
```

同理：`$\omega_2$`/`$\omega_3$`/`$\omega_4$`/`$b_1$`


- 推导：

对于输出层的神经元是`$n_8$`，有四个输入：

```math
\dfrac {\partial Err}{\partial \omega_{11}} = (o_6 - y)f(z_6)^{'}o_3
```
```math
\dfrac {\partial Err}{\partial \omega_{12}} = (o_6 - y)f(z_6)^{'}o_4
```
```math
\dfrac {\partial Err}{\partial \omega_{13}} = (o_6 - y)f(z_6)^{'}o_5
```
```math
\dfrac {\partial Err}{\partial b_{3}} = (o_6 - y)f(z_6)^{'}
```

其中`$b_3$`可以改成

```math
\dfrac {\partial Err}{\partial b_{3}} = (o_6 - y)f(z_6)^{'} 1
```

那么`$b_3$`也可以视为一种特殊的“权重”，那么`Err`对它们的一阶偏导分为两部分，一部分是这些常数，另一部分是`$(o_6 - y) f(z_6)^{'}$` 

**以`$n_8$`为中心，抽象而言又分别代表后续环节的“所有结果”和“自身”的关系**

`$n_8$`后续环节的“所有结果”可用一个“附加变量”，记录 `$(o_6 - y)f(z_6)^{'}$`

这样对于`$\omega_{11}$`、`$\omega_{12}$`、`$\omega_{13}$`、`$b_{3}$`,只需要关注`$n_8$`本身即可，不再需要考虑后续的环节，直接用其“附加变量”，可得一个重要结论：**对任意一个神经元而言，推导权重的迭代公式，可以形式上只跟“权重”的两端相关**

神经元`$n_5$`的“附加变量”为：`$(o_6 - y) f(z_6)^{'} \omega_{11} f(z_3)^{'}$`

所以`$\omega_5$`的一阶偏导为：`$n_5附加变量 o_1$`

同理：

神经元`$n_6$`的“附加变量”为：`$(o_6 - y) f(z_6)^{'} \omega_{12} f(z_4)^{'}$`

所以`$\omega_6$`的一阶偏导为：`$n_6附加变量 o_1$`

神经元`$n_7$`的“附加变量”为：`$(o_6 - y) f(z_6)^{'} \omega_{13} f(z_5)^{'}$`

所以`$\omega_7$`的一阶偏导为：`$n_7附加变量 o_1$`

对于`$b_2$`，是三条路径相加，即神经元`$n_5$`的“附加变量”、神经元`$n_6$`的“附加变量”、神经元`$n_7$`的“附加变量”

对于`$\omega_1$`的一阶偏导：

```math
\dfrac {\partial Err}{\partial \omega_{1}} = (o_6 - y)f(z_6)^{'} (\omega_{11} f(z_3)^{'} \omega_{5} + \omega_{12} f(z_4)^{'} \omega_{6} + \omega_{13} f(z_5)^{'} \omega_{7}) f(z_1)^{'} x_1
```

## 5.通用神经网络的BP算法推导

1. 神经网络的参数是各层神经元之间的权重和偏差
2. 偏差可以视为特殊的权重
3. **目标函数`Err`对权重的一阶偏导，只跟权重连接的两个神经元相关：前一个神经元的输出和后一个神经元“附加变量”的乘积**


表述方式：神经网络有`N+1`层，输入层是第`0`层，输出层是第`N`层

`$o_0 ^ N$`表示第`N`层的第`0`个神经元的输出，`$z_0^N$`表示第`N`层的第`0`个神经元的输入，`$D^N +1$`表示第`N`层第`0`个神经元的输出，`$\omega_{0,D^{N-1}}  ^ N$`表示第`$N-1$`层的第`$D^{N-1}$`个神经元和第`N`层的第`0`个神经元的权重


此时计算`Err`为：

![image.png](https://note.youdao.com/yws/res/32403/WEBRESOURCE0eaa24fbe15cd4e97e9642e514c56499)

此时对于 `$\dfrac {\partial Err}{\partial \omega_{j,i} ^ {m}}$`

![image.png](https://note.youdao.com/yws/res/32412/WEBRESOURCEf0eb8bcadc916176edffd31c62da8915)

![image.png](https://note.youdao.com/yws/res/32414/WEBRESOURCE156f504a134ee93361df67a202fa7629)

![image.png](https://note.youdao.com/yws/res/32416/WEBRESOURCE8e35d2cb9413b260b6acebaf098358b5)


# 自动微分

```math
y = f_{w,b}(x)
```

神经网络的训练，本质上就是“调整”这个函数的`$\omega$`和`b`，使得输入是`x`的时候，输出非常接近`y`

调整的方式就是微分求极值，如果模型很复杂，可用自动微分

## 反向推导微分

> 反向推导微分

**反向推导微分优于前向推导微分**

例：

```math
v_0 = 2 x_0
```
```math
v_1 = 3 x_1
```
```math
y = v_0 / v_1
```

反向推导：

![image.png](https://note.youdao.com/yws/res/32477/WEBRESOURCE3e78f231bacb7fb2d91d9012599d1320)

## 计算图

> 计算图

比如：

```math
v_0 = x_0
```
```math
v_1 = x_1
```
```math
v_2 = v_0 v_1
```
```math
v_3 = v_1 / v_0
```
```math
y = v_2 + v_3
```

![image.png](https://note.youdao.com/yws/res/32500/WEBRESOURCEa44874b1726a9507b1d317b4d622b961)

对其相应求偏导：

![image.png](https://note.youdao.com/yws/res/32507/WEBRESOURCE56237606a7428e9598ec872470a91b46)

使用`grad`记录它的梯度，即：

```math
v_1 grad = \dot{y} \mid {v_1} = \dfrac{\partial y}{\partial v_1}
```

如何计算？

![image.png](https://note.youdao.com/yws/res/32535/WEBRESOURCEa80a7e731fbc0461074eeb186141b9c5)

## c++实现

```cpp
#include <iostream>
#include <vector>

using namespace std;

enum class Operation
{
	AddOp,
	MulOp,
	DivOp,
	Equal,
	None
};

class Node {
public:
	Node() = default;
	Node(double value, double grad = 0.0, Operation op = Operation::None, vector<Node*> inputs = {})
		 : value_(value), grad_(grad), op_(op){
		for (auto c : inputs) {
			inputs_.push_back(c);
		}
	};

	double read_value() { return value_; }
	double read_grad() { return grad_; }

	void evaluate();
	void gradient();

private:
	double value_{ 0.0 };
	double grad_ { 0.0 };
	Operation op_{ Operation::None };
	vector<Node*> inputs_{};
};

int main() {
	Node x0(3);
	Node x1(5);
	Node v0(0.0, 0.0, Operation::Equal, vector<Node*>{&x0});
	Node v1(0.0, 0.0, Operation::Equal, vector<Node*>{&x1});
	Node v2(0.0, 0.0, Operation::MulOp, vector<Node*>{&v0, &v1});
	Node v3(0.0, 0.0, Operation::DivOp, vector<Node*>{&v1, & v0});
	Node y (0.0, 1, Operation::AddOp, vector<Node*>{&v2, & v3});


	v0.evaluate(); v1.evaluate(); v2.evaluate(); v3.evaluate(); y.evaluate();

	y.gradient(); v3.gradient(); v2.gradient(); v1.gradient(); v0.gradient();

	cout << "y.value = " << y.read_value() << endl;
	cout << "x0.grad = " << x0.read_grad() << endl;
	cout << "x1.grad = " << x1.read_grad() << endl;

}

void Node::evaluate() {
	if (op_ == Operation::AddOp) {
		value_ = inputs_[0]->value_ + inputs_[1]->value_;
	}
	else if (op_ == Operation::MulOp) {
		value_ = inputs_[0]->value_ * inputs_[1]->value_;
	}
	else if (op_ == Operation::DivOp) {
		value_ = inputs_[0]->value_ / inputs_[1]->value_;
	}
	else if (op_ == Operation::Equal) {
		value_ = inputs_[0]->value_;
	}
	else
		return;
}

void Node::gradient() {
	if (op_ == Operation::AddOp) {
		inputs_[0]->grad_ += grad_;
		inputs_[1]->grad_ += grad_;
	}
	else if (op_ == Operation::MulOp) {
		inputs_[0]->grad_ += grad_ * inputs_[1]->value_;
		inputs_[1]->grad_ += grad_ * inputs_[0]->value_;
	}
	else if (op_ == Operation::DivOp) {
		inputs_[0]->grad_ += grad_ / inputs_[1]->value_;
		inputs_[1]->grad_ += (- grad_ * inputs_[0]->value_) / (inputs_[1]->value_ * inputs_[1]->value_);
	}
	else if (op_ == Operation::Equal) {
		inputs_[0]->grad_ += grad_;
	}
	else
		return;
}
```

![image.png](https://note.youdao.com/yws/res/32539/WEBRESOURCEc5c3853a442955903b707b10d6374840)

# 深度学习之RNN

## 最简RNN

![image.png](https://note.youdao.com/yws/res/32703/WEBRESOURCEfc189a443f0d047278ec28660f906f8b)

但由于研究发现，这个黑箱计算输出值`s`的时候，会使用上一个时刻的`s`，即：如果`t`是时间点，计算`$s_{t+1}$`的时候，需要使用`$s_t$`的值

![image.png](https://note.youdao.com/yws/res/32728/WEBRESOURCEdd065e029595d3b3176d6128263849e8)

最简`RNN`有五个参数：`u`,`v`,`w`,`$b_f$`,`$b_g$`,用`BP`算法求解

假设有一对`t+1`时刻的训练样本，`$[x_{t+1},y_{t+1}]$`,其中`$x_{t+1}$`是输入，`$y_{t+1}$`是目标值，t时刻的`s`值`$s_t$`是已知的。`RNN`的输出是`$o_{t+1}$`，训练的目标是让`$o_{t+1}$`尽可能接近`$y_{t+1}$`

![image.png](https://note.youdao.com/yws/res/32781/WEBRESOURCE601a860045d59992ea95e2e0f95ed238)

一阶偏导推导：

![image.png](https://note.youdao.com/yws/res/32785/WEBRESOURCE7144976143c7feef37d944aea66671b8)

## `LSTM`的`BP`算法




# 五折交叉验证

![](https://img-blog.csdnimg.cn/20210428192922841.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQyNjQzNzM=,size_16,color_FFFFFF,t_70)

> 五折交叉验证

把数据平均分成五等份，每次实验拿一份做测试，其余用做训练，实验五次求平均值

使用`KFold`类：

- 调用`from sklearn.model_selection import KFold`
- 形式`KFold(n_splits = 5,shuffle = False, random_state = None)`
- 参数`n_splits`：表示划分为几块，`shuffle`：表示是否打乱划分，`random_state`：表示是否固定随机起点（当`shuffle`为`true`时使用）
- 方法`get_n_splits(groups)`：返交叉验证器中的拆分迭代次数
- 方法`split(groups)`：返回划分后数据集的`index`，将数据拆分为训练集和测试集

通过索引去获取数据和对应的标签可用：

```python
kf = KFold(n_split = 5,shuffle = False);

for train_index , test_index in kf.split(X)

fold1_train_data,fold1_train_label = X[train_index] , y[train_index]
```

![image.png](https://note.youdao.com/yws/res/32696/WEBRESOURCE9b83b547d892fa4b00bf813c422272cf)

# 安装`vcpkg`和`libtorch`

git 克隆 `https://github.com/microsoft/vcpkg.git`

运行`vcpkg`文件夹下的`bootstrap-vcpkg.bat`文件，获取最新的`vcpkg`可执行程序`vcpkg.exe`

在命令行运行以下命令将`vcpkg`集成到`Visual Studio`：`vcpkg.exe integrate install`

# `Pytorch`的`python`求解

> 实例：求解一个极值问题

```math
y = 1/2 (x-2)^2
```

> 实现

```python
from pickle import TRUE
import torch
import numpy as np

# x是一个张量Tensor ，只有一个元素，形状为（1，1），且服从均值为0，标准差为0.01的正态分布的随机数来初始化该张量
x = torch.tensor(np.random.normal(0,0.01,(1,1)),dtype = torch.float32)

# 需要计算x的梯度，以便对x进行优化
x.requires_grad_(requires_grad = True)

# 学习速率
eta = 0.4

for i in range (15):
    print('x =',x.data.item())
    # 计算损失函数 y
    y = (x-2) ** 2/2
    # 计算 y 关于 x 的梯度
    y.backward()
    x.data -= eta*x.grad
    # 将梯度重置为零，以便在下一次迭代中计算新的梯度
    x.grad.data.zero_()
```
![image.png](https://note.youdao.com/yws/res/33048/WEBRESOURCEde0fb19cd0d5167c8e7013bd233e63f1)

其中`y = (x-2) ** 2/2`为前向计算，计算图的前一半，`y.backward()`为反向计算梯度，计算图的后一半

> 例2：有一个向量`$[x_0,x_1]$`，求下面函数的极值

```math
y = x^2_0 + x^2_1 + 4x_0+5x_1+3
```

```python
from pickle import TRUE
import torch
import numpy as np

x = torch.tensor(np.random.normal(0,0.01,(1,2)),dtype = torch.float32)

x.requires_grad_(requires_grad = True)

# 学习速率
eta = 0.4

for i in range (10):
    print('x =',x)
    # 计算损失函数 y , x.t()：张量转置操作，torch.mm():矩形乘法
    y = torch.mm(x,x.t()) + 4 * x[0,0] + 5 * x[0,1] + 3;
    # 计算 y 关于 x 的梯度
    y.backward()
    x.data -= eta*x.grad
    # 将梯度重置为零，以便在下一次迭代中计算新的梯度
    x.grad.data.zero_()
```

![image.png](https://note.youdao.com/yws/res/33092/WEBRESOURCE9c3e52aefc21aac38fa4a93f11adba2a)

## 求解线性回归模型

> 例题1：求解 `y = wx + b`,且为二元线性回归

- 生成样本数据

```python
import torch

w = torch.tensor([3.1,4.2],dtype = torch.float32).unsqueeze(dim =0)
b = torch.tensor([0.5],dtype = torch.float32)
x = torch.tensor([[1.1,4.5,8.9],[2.3,5.7,10.1]],dtype = torch.float32)

y_target = torch.mm(w,x) + b
print('y_tatget =',y_target)
```

![image.png](https://note.youdao.com/yws/res/33119/WEBRESOURCE8aaf5c1f69506d9d2a5740c267a62754)

- 使用

```python
from asyncio import base_events
import torch

x = torch.tensor([[1.1,4.5,8.9],[2.3,5.7,10.1]],dtype = torch.float32)
y_target = torch.tensor([13.5700,38.7000,70.5100],dtype = torch.float32)

w = torch.tensor([0.001,0.003],dtype = torch.float32).unsqueeze(dim=0)
w.requires_grad_(requires_grad = True)
b = torch.tensor([0.005],dtype = torch.float32)
b.requires_grad_(requires_grad = True)

eta = 0.0001
for i in range(30000):
    print('-'*200)
    print('w = ',w)
    print('b = ',b)
    y = torch.mm(w,x) + b
    print('y = ',y)
    loss = (y - y_target)** 2/2
    print('loss = ',loss)
    sum_err = torch.sum(loss)
    print('sum_err = ',sum_err)
    sum_err.backward()
    print('w.grad = ',w.grad)
    w.data -= eta*w.grad
    b.data -= eta*b.grad
    w.grad.data.zero_()
    b.grad.data.zero_()
```

![image.png](https://note.youdao.com/yws/res/33128/WEBRESOURCE7dd3467b54deb3a7d10b50a13dbbeef6)

## 多层

神经网络的结构：一个输入层，若干个隐层，一个输出层
每一层跟其他层之间的连接方式：每一层的激活函数，目标函数

```python
from xml.dom.minidom import DocumentType
import torch
from torch import nn

# 每层神经元数量
n_inputs,n_hiddens_1,n_hiddens_2,n_outputs = 2,3,4,1

# 定义网络结构：各层属性和激活函数
net = nn.Sequential(nn.Linear(n_inputs,n_hiddens_1),nn.ReLU(),
                    nn.Linear(n_hiddens_1,n_hiddens_2),nn.ReLU(),
                    nn.Linear(n_hiddens_2,n_outputs))

# 初始化参数
for params in net.parameters():
    torch.nn.init.normal_(params,mean = 0,std = 0.01)

# 优化器
optimizer = torch.optim.SGD(net.parameters(),lr = 0.5)

# 样本
x = torch.tensor([2.1,3.2],dtype = torch.float32).unsqueeze(dim = 0)
y_target = torch.tensor([0.7],dtype = torch.float32).unsqueeze(dim = 0)

# 训练
for i in range(5):
    print('-' * 20)
    y = net(x)
    print('y = ', y)
    l = (y - y_target) ** 2/2
    l.backward()
    print('l = ', l)
    optimizer.step()
    optimizer.zero_grad()
```

![image.png](https://note.youdao.com/yws/res/33154/WEBRESOURCE8a98cc2346c2159ef8530b1416b90a4d)

## 实现一个最简单的循环神经网络`rnn`




